{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28e6f30-b299-40ef-8d3e-019be5aa0fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/18 12:19:52 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from model-eskperimen5/content/output/model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duevano/miniconda3/envs/detectron2-env/lib/python3.11/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success, image saved\n"
     ]
    }
   ],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import json, random\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"model-eskperimen5/configmodel5.yml\")\n",
    "cfg.MODEL.WEIGHTS = \"model-eskperimen5/content/output/model_final.pth\"\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55   # set the testing threshold for this model (0.6)\n",
    "# cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
    "#cfg.DATASETS.TEST = (\"my_dataset_val\", )\n",
    "\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "imageName = \"./kemejadanchinosRGB.png\"\n",
    "im = cv2.imread(imageName)\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:, :, ::-1], scale=0.8)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "# cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "# print(\"success\")\n",
    "\n",
    "output_image = out.get_image()[:, :, ::-1]  # Convert RGB to BGR\n",
    "cv2.imwrite(\"./output_prediction.png\", output_image)\n",
    "print(\"success, image saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90762436-d310-4089-8967-54a827c65d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duevano/miniconda3/envs/detectron2-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2Processor\n",
    "\n",
    "# load the pretrain model?\n",
    "checkpoint = 'Salesforce/blip2-opt-2.7b'\n",
    "processor = Blip2Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca9442eb-1c50-41f1-86f0-5f60bbedaf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import Blip2ForConditionalGeneration\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BASE model hanya sekali\n",
    "base_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"ybelkada/blip2-opt-2.7b-fp16-sharded\",  # sesuaikan dengan base dari semua peft_model_id kamu\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"./cache-blip2\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af7f7ee1-b8a8-4eb7-827e-a8d3222551d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk switch adapter\n",
    "def load_adapter(base_model, peft_model_id, device):\n",
    "    model = PeftModel.from_pretrained(base_model, peft_model_id)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "# Contoh penggunaan:\n",
    "# Ganti adapter dengan cepat\n",
    "peft_model_id_1 = \"content/data/blip2_visual_model_1v2_nonpadded\"\n",
    "peft_model_id_2 = \"content/data/blip2_visual_model_1v2\"\n",
    "\n",
    "# Ganti adapter sesuai kebutuhan\n",
    "# model = load_adapter(base_model, peft_model_id_1, device)\n",
    "\n",
    "# ... lakukan inferensi\n",
    "model = load_adapter(base_model, peft_model_id_2, device)\n",
    "# ... lakukan inferensi lagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421b5ecf-41a6-4ba9-9762-a1ee72a6d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set API key for OPENAI LLM\n",
    "os.environ[\"OPENAI_API_KEY\"] = '' # masukkan api key LLM, bisa pakai key dari LLM berbayar atau key gratis dari github models\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29fab5e7-2003-442f-8dc8-d269b9aee1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# max length caption generated by blip-2\n",
    "MAX_LENGTH = 16\n",
    "# batch size for blip-2 model\n",
    "CAPTION_BATCH_SIZE = 32  # number of crops per batch for BLIP-2 inference\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Specify which segmentation classes to include (remember! index start from 0)\n",
    "CHOSEN_CLASSES = [4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def segment_image_in_memory(image: np.ndarray):\n",
    "    \"\"\"\n",
    "    Perform instance segmentation and return list of crop arrays and metadata, without writing to disk.\n",
    "    \"\"\"\n",
    "    outputs = predictor(image)\n",
    "    instances = outputs[\"instances\"].to(\"cpu\")\n",
    "    masks = instances.pred_masks.numpy()\n",
    "    boxes = instances.pred_boxes.tensor.numpy().astype(int)\n",
    "    classes = instances.pred_classes.numpy()\n",
    "    scores = instances.scores.numpy()\n",
    "    crops_meta = []\n",
    "    for i, (mask, box, cls, score) in enumerate(zip(masks, boxes, classes, scores)):\n",
    "        # if cls not in CHOSEN_CLASSES: continue # skip crops if filtered\n",
    "        x1, y1, x2, y2 = box\n",
    "        crop = image[y1:y2, x1:x2]\n",
    "        mask_crop = (mask[y1:y2, x1:x2] * 255).astype(np.uint8)\n",
    "        crop_masked = cv2.bitwise_and(crop, crop, mask=mask_crop)\n",
    "        pil_img = Image.fromarray(cv2.cvtColor(crop_masked, cv2.COLOR_BGR2RGB))\n",
    "        crops_meta.append({\"image\": pil_img, \"class\": int(cls), \"score\": float(score)})\n",
    "    return crops_meta\n",
    "\n",
    "\n",
    "def batch_caption_inference(crops_meta, processor, model, device, max_length, batch_size):\n",
    "    \"\"\"\n",
    "    Run BLIP-2 captioning on crops in batches. Returns list of captions aligned to crops_meta.\n",
    "    \"\"\"\n",
    "    captions = []\n",
    "    for i in range(0, len(crops_meta), batch_size):\n",
    "        batch = crops_meta[i:i+batch_size]\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device, torch.float16)\n",
    "        out_ids = model.generate(pixel_values=inputs.pixel_values, max_length=max_length)\n",
    "        batch_caps = processor.batch_decode(out_ids, skip_special_tokens=True)\n",
    "        captions.extend(batch_caps)\n",
    "    return captions\n",
    "\n",
    "\n",
    "def combine_instance_captions(captions_list):\n",
    "    # remove empty and duplicates\n",
    "    unique = list(dict.fromkeys([c for c in captions_list if c and c.strip()]))\n",
    "    return \", \".join(unique)\n",
    "\n",
    "# LLM combination (optional)\n",
    "def llm_combine(caption_str: str):\n",
    "    # <A> no punctuation, no \"person is wearing\" => ideal for scoring\n",
    "    # message_content = (\n",
    "    #     \"I have a list of captions that describe each segmented part of a fully clothed person in an image. \"\n",
    "    #     \"The list of captions is: \" + caption_str + \". \"\n",
    "    #     \"Your task is to combine these captions into one cohesive caption that describes \"\n",
    "    #     \"the clothing worn by the person in the image concisely, clearly, and briefly as humanly said as possible. \"\n",
    "    #     \"Just provide the final caption directly and please responds only using lowercase letter without using any punctuation. \"\n",
    "    #     \"Remember to remove any elements from the list that you think are duplicates, for example: if there are two list members that mention sneakers then ignore one of them.\"\n",
    "    # )\n",
    "    \n",
    "    # <B> with punctuation, with \"person is wearing\" => deprecated i guess?\n",
    "    message_content = (\n",
    "        \"I have a list of captions that describe each segmented part of a fully clothed person in an image. \"\n",
    "        \"The list of captions is: \" + caption_str + \". \"\n",
    "        \"Your task is to combine these captions into one cohesive caption that describes \"\n",
    "        \"the clothing worn by the person in the image as humanly said as possible. \"\n",
    "        \"Just provide the final caption directly and please responds only using lowercase letter. \"\n",
    "        \"Remember to remove any elements from the list that you think are duplicates, for example: if there are two list members that mention sneakers then ignore one of them.\"\n",
    "    )\n",
    "    \n",
    "    # # <C> with punctuation, no \"person is wearing\" => ideal for questionnaire\n",
    "    # message_content = (\n",
    "    #     \"I have a list of captions that describe each segmented part of a fully clothed person in an image. \"\n",
    "    #     \"The list of captions is: \" + caption_str + \". \"\n",
    "    #     \"Your task is to combine these captions into one cohesive caption that describes \"\n",
    "    #     \"the clothing worn by the person in the image concisely, clearly, and briefly as humanly said as possible. \"\n",
    "    #     \"Just provide the final caption directly and please responds only using lowercase letter\"\n",
    "    #     \"Remember to remove any elements from the list that you think are duplicates, for example: if there are two list members that mention sneakers then ignore one of them.\"\n",
    "    #     \"Please do not use phrase like 'the person is wearing..', instead please combine the provided captions only using necessary conjunctions\"\n",
    "    # )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant which masters text rewriting\"},\n",
    "            {\"role\": \"user\", \"content\": message_content}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content.strip() # aku tambahain strip() untuk menghapus extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a36984e-0455-413c-a8c4-779e63e7b449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions per segment:\n",
      "- black jeans with a zipper down the leg and a belt around the\n",
      "- red brown sunglasses\n",
      "- brown sweater with a white collar and collarless sweater with a white\n",
      "- black white boston boston boston boston boston b\n",
      "- white red viper's wing collar fang necklace\n",
      "- black white canvas converse all star sneaker\n",
      "\n",
      "Combined (manual): black jeans with a zipper down the leg and a belt around the, red brown sunglasses, brown sweater with a white collar and collarless sweater with a white, black white boston boston boston boston boston b, white red viper's wing collar fang necklace, black white canvas converse all star sneaker\n",
      "\n",
      "Combined (LLM): the person is wearing black jeans with a zipper down the leg and a belt, a brown sweater featuring a white collar, red brown sunglasses, a white and red viper's wing collar fang necklace, and black and white canvas converse all star sneakers.\n",
      "\n",
      "Total time taken: 20.96 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# === LOAD PREDICTOR, PROCESSOR, DAN MODEL BLIP-2 SEBELUM INI ===\n",
    "# Misalnya:\n",
    "# predictor = ... # Mask R-CNN atau Detectron2 predictor\n",
    "# processor = ... # BLIP-2 processor (dari transformers)\n",
    "# model = ...     # BLIP-2 model\n",
    "# client = ...    # OpenAI client\n",
    "\n",
    "# --- Awal pengukuran waktu ---\n",
    "start_time = time.time()\n",
    "\n",
    "# === INPUT GAMBARNYA ===\n",
    "# image_path = \"kemejadanchinosRGB.png\" # gambar 1\n",
    "# image_path = \"inferenceblip2/435851fb-894a-4030-a981-afe9ad4c3502.jpeg\" # gambar 2\n",
    "# image_path = \"inferencepemaduancaption/image_0672.jpg\" # gambar 3\n",
    "# image_path = \"inferencepemaduancaption/0207933.jpg\" # gambar 4\n",
    "image_path = \"inferencepemaduancaption/1091631.jpg\" # gambar 5\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# === 1. Segmentasikan gambar ===\n",
    "crops_meta = segment_image_in_memory(image)\n",
    "\n",
    "# === 2. Lakukan captioning pada tiap hasil crop ===\n",
    "captions = batch_caption_inference(\n",
    "    crops_meta=crops_meta,\n",
    "    processor=processor,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    max_length=MAX_LENGTH,\n",
    "    batch_size=CAPTION_BATCH_SIZE\n",
    ")\n",
    "\n",
    "# === 3. Gabungkan caption (opsional, jika ingin satu kalimat deskriptif) ===\n",
    "combined_caption = combine_instance_captions(captions)\n",
    "\n",
    "# --- Awal pengukuran waktu ---\n",
    "# start_time = time.time()\n",
    "\n",
    "# Jika ingin gabungkan dengan LLM (opsional)\n",
    "final_caption = llm_combine(combined_caption)\n",
    "\n",
    "# --- Akhir pengukuran waktu ---\n",
    "end_time = time.time()\n",
    "total_duration = end_time - start_time\n",
    "\n",
    "# === OUTPUT ===\n",
    "print(\"Captions per segment:\")\n",
    "for cap in captions:\n",
    "    print(\"-\", cap)\n",
    "\n",
    "print(\"\\nCombined (manual):\", combined_caption)\n",
    "print(\"\\nCombined (LLM):\", final_caption)\n",
    "print(f\"\\nTotal time taken: {total_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34752cf9-7684-424e-bc8b-393678273ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
